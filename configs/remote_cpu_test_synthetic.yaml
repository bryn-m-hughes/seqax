# Command to run:
#   python -m train --config-name=remote_cpu_test_synthetic

defaults:
- base
- _self_

training:
  warmup_steps: 10
  steps:        50
  steps_for_lr: 100
  tokens:
    batch: 64
    len: 64
  queue: '0c4c80fa11774fd79efb60c7d8e16e52'

model:
  d_model: 256
  n_q_per_kv: 2
  n_kv: 2
  d_head: 32
  layers: 2
  vocab: 1280
  d_ff: 1024
  d_visual: 512
  rope_max_timescale: 256

paths:
  root_working_dir: 'gs://lam-train-artifacts'
  model_name: 'synthetic_000'

checkpoint_interval: 10
num_hosts: 1

mesh:
  d: 4
  t: 2

flat_tokens:
  filespec: 'synthetic_vl_dataset.zarr'
  streams: 2
  read_blocks_per_shuffle_buffer: 8
  sequences_per_read_block: 16
  seed: 0
  sequence_packing: true